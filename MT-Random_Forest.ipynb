{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random as rd\n",
    "import time\n",
    "from collections import defaultdict \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Nos modules\n",
    "import randomforest as rf \n",
    "import params_tuning as pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importation des données"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On supprime la premiere colonne\n",
    "data = data.drop(['filename'],axis=1)\n",
    "\n",
    "# On change les noms des genres par des entiers (de 0 a 9) car notre random forest ne prend en compte que des entiers comme labels\n",
    "genre_list = data.iloc[:, -1]\n",
    "encoder = LabelEncoder()\n",
    "labels = encoder.fit_transform(genre_list)\n",
    "data.iloc[:, -1] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On normalise le dataset\n",
    "scaler = StandardScaler()\n",
    "data_normalized = scaler.fit_transform(np.array(data.iloc[:, :-1], dtype = float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Apprentissage du modèle de base\n",
    "On reprend le modèle que nous avions implementé lors du précédent projet."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On separe le dataset en train set et test set (80%/20%)\n",
    "data_train, data_test, label_train, label_test = train_test_split(data_normalized, labels, test_size=0.2)\n",
    "print(\"data_train {0} | label_train {1}\".format(data_train.shape, label_train.shape))\n",
    "print(\"data_test  {0} | label_test  {1}\".format(data_test.shape, label_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation de la random forest\n",
    "# par defaut n_trees = 200, n_samples = 100, n_cuts = 20, max_depth = 20\n",
    "rf_classifier = rf.OurRandomForestClassifier() \n",
    "\n",
    "# Entrainement du modèle de base\n",
    "start = time.time()\n",
    "rf_classifier.fit(data_train, label_train)\n",
    "end = time.time()\n",
    "print(\"Execution time for building the forest: %f sec\"%(float(end) - float(start)))\n",
    "\n",
    "# Test de validation\n",
    "our_predictions = [rf_classifier.predict(data_test[i,:]) for i in range(data_test.shape[0])] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison avec sklearn\n",
    "sklearn_rf = RandomForestClassifier(n_estimators=100, max_depth=20, max_features='sqrt')\n",
    "sklearn_rf.fit(data_train, label_train)\n",
    "sklearn_predictions = sklearn_rf.predict(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Our random forest score : {} %\".format(rf_classifier.score(our_predictions, label_test) * 100))  \n",
    "print(\"Sklearn score : {} %\".format(rf_classifier.score(sklearn_predictions, label_test)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il y a une très grande marge d'amélioration."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature selection\n",
    "Un moyen d'améliorer notre modèle est de sélectionner les caractéristiques les plus discriminantes. \n",
    "\n",
    "* https://towardsdatascience.com/de-coding-random-forests-82d4dcbb91a1\n",
    "* https://hub.packtpub.com/4-ways-implement-feature-selection-python-machine-learning/"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of the dataset \", data_train.shape)\n",
    "print(\"Size of the dataset before feature selection: %.2f MB\"%(data_train.nbytes/1e6))\n",
    "features_name = data.drop(['genre/label'], axis=1).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "features, importances = rf_classifier.findFeatureImportance(data_train, label_train)\n",
    "end = time.time()\n",
    "print(\"Execution time to find the most important feature: %f sec\"%(float(end) - float(start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = pd.DataFrame(zip(features_name, importances), columns = ['feature','importance']).sort_values('importance', ascending=False)\n",
    "\n",
    "feature_importances[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On selectionne les 25 meilleures\n",
    "indexes = feature_importances.index[:25]\n",
    "\n",
    "# On transforme le dataset d'entrainement (fs = feature selection)\n",
    "fs_data_train = rf.transform(data_train, indexes)\n",
    "fs_data_test = rf.transform(data_test, indexes)\n",
    "print(\"Shape of the dataset \", fs_data_train.shape)\n",
    "print(\"Size of the dataset after feature selection: %.2f MB\"%(fs_data_train.nbytes/1e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrainement du modèle de base avec feature selection\n",
    "start = time.time()\n",
    "rf_classifier.fit(fs_data_train, label_train)\n",
    "end = time.time()\n",
    "print(\"Execution time for building the forest: %f sec\"%(float(end) - float(start)))\n",
    "\n",
    "# Test de validation\n",
    "our_predictions = [rf_classifier.predict(fs_data_test[i,:]) for i in range(data_test.shape[0])] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison avec sklearn\n",
    "sklearn_rf.fit(fs_data_train, label_train)\n",
    "sklearn_predictions = sklearn_rf.predict(fs_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score\n",
    "print(\"Our random forest score after feature selection: {} %\".format(rf_classifier.score(our_predictions, label_test) * 100))\n",
    "print(\"Sklearn score : {} %\".format(rf_classifier.score(sklearn_predictions, label_test)*100))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Réglage des hyperparamètres\n",
    "Nous nous sommes fortement inspiré de la méthode décrite dans cet article [W. Koehrsen. Hyperparameter Tuning the Random Forest in Python, Janv. 2018](https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notre modèle a cinq paramètres, dont quatre que nous souhaitons optimiser  :\n",
    "   - `n_trees` -- le nombre d'arbres de la forêt\n",
    "   - `n_samples` -- le nombre de données à placer dans le noeud de chaque arbre avant qu'il ne soit partitionné\n",
    "   - `n_cuts` -- le nombre de coupes à tester pour trouver la meilleure\n",
    "   - `max_depth` -- la profondeur maximale de chaque arbre"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour avoir une première idée de la meilleure combinaison d'hyperparamètres, nous allons effectuer une ... (Random Search Cross Validation). Cela consiste à tester un large choix de combinaisons qui ont été formées en tirant aléatoirement des valeurs dans une grille d'hyperparamètres."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Random Search Cross Validation"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Hyperparameter Grid"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On définit la grille pour la recherche aléatoire (Random Hyperparameter Grid) :"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_trees\n",
    "n_trees = [int(x) for x in np.linspace(start = 200, stop = 600, num = 5)] \n",
    "\n",
    "# n_samples\n",
    "n_samples = [int(x) for x in np.linspace(start = 200, stop = 500, num = 4)] # A GARDER OU PAS ?\n",
    "\n",
    "# n_cuts \n",
    "# dans quel intervalle pourrait on tester ??\n",
    "n_cuts = [int(x) for x in np.linspace(start = 10, stop = 50, num = 5)] \n",
    "\n",
    "# max_depth\n",
    "max_depth = [int(x) for x in np.linspace(10, 100, num = 10)]\n",
    "# Est-ce qu'on ajoute un None comme dans l'article ? A voir selon notre random forest, est-ce qu'elle prend en compte\n",
    "# un arg None pour max_depth ?\n",
    "\n",
    "# Creation de la grille\n",
    "random_grid = {'n_trees': n_trees,\n",
    "                'n_samples': n_samples,\n",
    "                'n_cuts': n_cuts,\n",
    "                'max_depth': max_depth}\n",
    "\n",
    "from pprint import pprint\n",
    "print(\"Grille d'hyperparametres :\\n\")\n",
    "pprint(random_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Au lieu de tester 10 x 10 x 5 x 10 = 5000 combinaisons d'hyperparamètres, nous allons seulement en sélectionner quelques unes aléatoirement."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Search Training"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On procède à la recherche randomisée sur 50 combinaisons, en utilisant une 3-fold CV"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NE PAS RUN, JE N'AI PAS ENCORE TESTE CAR CA PREND ENORMEMENT DE TEMPS (> 30 MIN VOIRE PLUS)\n",
    "# Definition de la recherche randomisee\n",
    "rf_random = pt.RandomizedSearchCV(estimator = rf.OurRandomForestClassifier, \n",
    "                                  param_distributions = random_grid, \n",
    "                                  n_iter = 5, \n",
    "                                  cv = 3)\n",
    "\n",
    "# Entrainement du modele\n",
    "start = time.time()\n",
    "rf_random.fit(data_train, label_train)\n",
    "end = time.time()\n",
    "print(\"Execution time for random search training: %f sec\"%(float(end) - float(start)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les resultats"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"La meilleure combinaison d'hyperparametres avec la recherche randomisee est :\")\n",
    "print(rf_random.best_params_)\n",
    "print(\"\")\n",
    "print(\"Le score moyen du modele avec ces hyperparametres est :\")\n",
    "print(rf_random.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Grid Search Cross Validation\n",
    "Une fois qu'on connait a peu pres les meilleurs hyper-parametres\n",
    "Plus d'aleatoire, on teste toutes les combinaisons"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit",
   "language": "python",
   "name": "python37664bit130b65de45ae4ae8a1d5066beb0585a6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}